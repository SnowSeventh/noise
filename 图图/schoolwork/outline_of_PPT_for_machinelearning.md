# Outline
***- 大纲草稿不包括开场白结束语等类似的信息***
## 第一部分: q-learning
1. 引入迷宫问题，挂出来迷宫的拓扑图，讲明游戏规则，以及今天我们要以它为背景来讨论q-learning和DQN；
2. 简单介绍reward矩阵和q矩阵以及action矩阵，稍稍强调下该问题之下的q矩阵，同时回顾老师课上求max（q）的理论依据；
3. 这部分简单的理论讲解结束后，快速闪过我们的代码，让大家对q-learning这部分代码诸如贴现率r，迭代次数的参数有个印象，同时呈现下代码的输出结果，即成型的q矩阵；

## 第二部分： 过渡
4. 这个问题的拓扑结构过于简单，6个不定态好解决。但是一旦态数过多，传统的q-learning解法将失去意义，因为消耗的算力实际无法提供；
5. 回顾老师课上对于这个问题的建议：问题的本质在于找到一个赋范线性空间下的泛函：$f:=(s,a,s',r) ->(q1,q2.....)$，使得我们可以通过输入不同的态进而得到该态对应下所有的q值。一个简单的思路是老师课上提到的线性函数估值：人为设定n个特征函数作为赋范线性空间的向量基，然后调整不同基底的权值(w1,w2......)，进而得到q函数。但是这种做法不确定性极大，q函数的质量完全取决于向量基的选取。
6. 与此同时，神经网络由于其特殊的属性，可以通过训练进而模拟出任意形式的映射关系，可以很方便的得到我们所需要的q函数，所以DQN必然可以很好的解决这类不定态的问题，也是我们今天研究的重点。

## 第三部分： DQN
### 准备工作
7. 首先，这个问题我们采用了如下结构的神经网络：3层，含一层隐藏层。输入层含有6个元，表示可能存在的六个不同的状态，即输入层输入的是不同的态矩阵s；输出层也含有6个元，表示每个态s可能存在的6个不同的q值，即输出单一态的q向量。这里用神经网络的结构图结合讲解；
8. 此外，传统的神经网络在验证输出的loss大小时，通常会用正确解和训练解做半方差，来衡量误差。这里我们的target q值并没有标准的参照，因为q矩阵本身未知。所以，q值来源于神经网络对态s的训练，而target q来自于神经网络对态s'的训练，再用的到的s'的q值利用:

    $Q(s, a) = r + Gamma * Max[Q(s’, all actions)]$

    得到target q，进而，$loss = (target_q - q)^2$

    正如同老师在课上的演示过程一样，得到理想q矩阵的过程就是这两个不同的q值在无限逼近的过程，直到收敛成为一个矩阵。

9. 最后，关于训练集的来源，我们采用了**部分随机，部分贪婪**的策略，即设定一个参数$θ$:按均匀分布在区间[0,1)随机取值，当取到的值小于$θ$时，随机从迷宫中任意的一个态进行任意一个可以执行的动作，并将(s,a,s',r,terminal)这五个数据存入初始化为空的记忆池；反之，当取到的值大于$θ$时，将当前态直接传入神经网络，得到q值列表后，去q值最大的行为，像上面一样存入记忆池。如此迭代进行数万步，直到记忆池足够大。当记忆池足够大后，逐步减小$θ$，使得随机游走的行为停止。这样，当训练神经网络的时候，定量的从记忆池中去一定量的样本，逐步训练即可。这里应该多插入些图片帮助理解。
### 正式流程
10. 初始化，所有参数和神经网络的初始化构建，截图截代码给观众；

11. 进入程序的循环遍历部分，即一遍生成记忆池，一边训练，具体包括以下几个步骤：
  - 12. 随机选取一个状态，并按照部分随机部分贪婪的策略，选取一个动作，将s,a,s',r,terminal这五个参量存入记忆池；

  - 13. 如果记忆池已满，那么从记忆池调取训练数据，进行训练；否则，跳转到14；

  - 14. 记忆池未满，那么当没有走出迷宫时，按照策略赋给程序下一个动作；否则，给程序随机设置一个新状态，继续运动。

    这几步都要结合代码的截图简要说明。

15. 截图展示训练结果，和q-learning进行对比。
## 第四部分：总结
16. DQN在处理态数巨大的不定态问题是十分高效，这里我们为了清晰阐述该理论，只选取了态数很少的用例。
17. 为了让大家感受DQN的距离作用，把别人的项目的视频放出来。
